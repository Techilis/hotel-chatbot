{"cells":[{"cell_type":"markdown","metadata":{"id":"6nEJWqJ9iATt"},"source":["### 1. Data Preprocessing (Cleaning Reviews)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0G2GzisiATu"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import langid\n","\n","df = pd.read_csv(\"booking_reviews_combined.csv\", encoding=\"utf-8\")\n","print(f\"Number of reviews before data preprocessing: {len(df)}\")\n","\n","# --------------------------\n","# 1. Detect language\n","# --------------------------\n","# Fill N/A\n","df['review_title'] = df['review_title'].fillna('')\n","df['review_positive_text'] = df['review_positive_text'].fillna('')\n","df['review_negative_text'] = df['review_negative_text'].fillna('')\n","\n","# Detect language\n","df['detected_lang_title'] = df['review_title'].apply(lambda x: langid.classify(str(x))[0] if x.strip() else np.nan)\n","df['detected_lang_pos'] = df['review_positive_text'].apply(lambda x: langid.classify(str(x))[0] if x.strip() else np.nan)\n","df['detected_lang_neg'] = df['review_negative_text'].apply(lambda x: langid.classify(str(x))[0] if x.strip() else np.nan)\n","\n","file_path = 'booking_reviews_detected_lang.csv'\n","df.to_csv(file_path, index=False, encoding=\"utf-8\")\n","print(f\"Translated reviews saved to {file_path}.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvZDuL-6iATw","outputId":"2386c45e-889a-4c10-b839-d06691a6d518"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of reviews before cleaning: 292507\n","Number of foreign language reviews: 78710, making up 26.91% of the original data\n","\n","‚úÖ Cleaned data saved: 185125 rows (63.29% of original)\n","üóëÔ∏è Removed rows saved: 107382 rows (36.71% of original)\n"]}],"source":["# --------------------------\n","# 0. Load data and initialize\n","# --------------------------\n","file_path = 'booking_reviews_detected_lang.csv'\n","df = pd.read_csv(file_path, encoding=\"utf-8\")\n","df[\"removal_reason\"] = None\n","total_rows = len(df)\n","print(f\"Total number of reviews before cleaning: {total_rows}\")\n","\n","# Backup original text before cleaning\n","original_text_cols = df[[\"review_positive_text\", \"review_negative_text\"]].copy()\n","\n","# --------------------------\n","# 1. Remove foreign-language reviews\n","# --------------------------\n","foreign_lang_mask = (\n","    ((df['detected_lang_pos'].notna() & (df['detected_lang_pos'] != 'en')) &\n","     (df['detected_lang_neg'].notna() & (df['detected_lang_neg'] != 'en')))\n","    |\n","    ((df['detected_lang_pos'].notna() & (df['detected_lang_pos'] != 'en')) &\n","     (df['detected_lang_neg'].isna()))\n","    |\n","    ((df['detected_lang_neg'].notna() & (df['detected_lang_neg'] != 'en')) &\n","     (df['detected_lang_pos'].isna()))\n",")\n","df.loc[foreign_lang_mask, \"removal_reason\"] = \"Foreign language\"\n","num_foreign = foreign_lang_mask.sum()\n","print(f\"Number of foreign language reviews: {num_foreign}, making up {num_foreign/total_rows:.2%} of the original data\")\n","\n","# --------------------------\n","# 2. Define text cleaning functions\n","# --------------------------\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return np.nan\n","    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", text)\n","    text = re.sub(r\"\\.\\s+\", \". \", text)\n","    return text.strip() if text.strip() else np.nan\n","\n","GENERIC_WORDS = {\n","    # Positive\n","    \"good\", \"very good\", \"all good\", \"all ok\", \"nice\", \"excellent\", \"enjoyable\", \"great\", \"ok\", \"fine\",\n","    \"decent\", \"amazing\", \"awesome\", \"perfect\", \"super\", \"satisfactory\", \"satisfying\",\n","    \"wonderful\", \"fantastic\", \"best\", \"cool\", \"superb\", \"nice place\", \"happy\", \"alright\",\n","    # Negative\n","    \"nothing\", \"none\", \"no\", \"n/a\", \"na\", \"nil\", \"no comments\", \"no complaint\", \"no issues\",\n","    \"not bad\", \"not good\", \"neutral\", \"meh\", \"so so\", \"average\", \"passable\", \"okay\", \"fair\"\n","}\n","\n","def clean_reviews(df, cols, generic_words, min_words=5):\n","    pattern = \"|\".join(map(re.escape, generic_words))\n","\n","    for col in cols:\n","        df[col] = df[col].apply(clean_text)\n","\n","        is_generic = df[col].str.lower().str.fullmatch(pattern, na=False)\n","        df.loc[is_generic, col] = np.nan\n","\n","        is_short = df[col].str.split().str.len() < min_words\n","        df.loc[is_short, col] = np.nan\n","\n","    return df\n","\n","# --------------------------\n","# 3. Clean short/generic reviews\n","# --------------------------\n","df = clean_reviews(df, [\"review_positive_text\", \"review_negative_text\"], GENERIC_WORDS)\n","\n","# --------------------------\n","# 4. Remove rows with no valid text\n","# --------------------------\n","empty_mask = df['review_positive_text'].isna() & df['review_negative_text'].isna()\n","df.loc[empty_mask & df[\"removal_reason\"].isna(), \"removal_reason\"] = \"No valid text\"\n","\n","# --------------------------\n","# 5. Split cleaned vs removed\n","# --------------------------\n","df_cleaned = df[df[\"removal_reason\"].isna()].copy()\n","removed_rows = df[df[\"removal_reason\"].notna()].copy()\n","\n","# Restore original review text in removed rows\n","removed_rows[[\"review_positive_text\", \"review_negative_text\"]] = original_text_cols.loc[removed_rows.index]\n","\n","# --------------------------\n","# 6. Save outputs and print summary\n","# --------------------------\n","df_cleaned.to_csv(\"booking_reviews_clean.csv\", index=False)\n","removed_rows.to_csv(\"booking_reviews_filtered_out.csv\", index=False)\n","\n","pct_kept = len(df_cleaned) / total_rows\n","pct_removed = len(removed_rows) / total_rows\n","\n","print(f\"\\n‚úÖ Cleaned data saved: {len(df_cleaned)} rows ({pct_kept:.2%} of original)\")\n","print(f\"üóëÔ∏è Removed rows saved: {len(removed_rows)} rows ({pct_removed:.2%} of original)\")\n"]},{"cell_type":"markdown","metadata":{"id":"zjXY4lgoiATz"},"source":["### 2. Chunking Reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwqdOLYjiATz","outputId":"a1d7a712-2833-4100-bbd5-6ec6393c64a6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_ID</th>\n","      <th>reviewer_name</th>\n","      <th>reviewer_country</th>\n","      <th>review_room_name</th>\n","      <th>review_num_nights</th>\n","      <th>review_stay_date</th>\n","      <th>review_traveler_type</th>\n","      <th>review_score</th>\n","      <th>review_title</th>\n","      <th>review_positive_text</th>\n","      <th>review_negative_text</th>\n","      <th>hotel_name</th>\n","      <th>detected_lang_title</th>\n","      <th>detected_lang_pos</th>\n","      <th>detected_lang_neg</th>\n","      <th>removal_reason</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Gabriela</td>\n","      <td>United States</td>\n","      <td>Deluxe Twin, Window</td>\n","      <td>2</td>\n","      <td>Jan-25</td>\n","      <td>Family</td>\n","      <td>10.0</td>\n","      <td>Comfortable and convenient</td>\n","      <td>This was a last minute booking so we arrived t...</td>\n","      <td>NaN</td>\n","      <td>A Hotel Joo Chiat</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>A</td>\n","      <td>Singapore</td>\n","      <td>Superior Double, No Window</td>\n","      <td>4</td>\n","      <td>Feb-25</td>\n","      <td>Couple</td>\n","      <td>7.0</td>\n","      <td>Overall I'm quite happy with the stay except f...</td>\n","      <td>NaN</td>\n","      <td>A little costly for a budget hotel.</td>\n","      <td>A Hotel Joo Chiat</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Sumarni</td>\n","      <td>Australia</td>\n","      <td>Superior Twin, No Window</td>\n","      <td>3</td>\n","      <td>Feb-25</td>\n","      <td>Group</td>\n","      <td>8.0</td>\n","      <td>Good and achieve my expectation</td>\n","      <td>NaN</td>\n","      <td>The fire alarm and no communication with the p...</td>\n","      <td>A Hotel Joo Chiat</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Samsul</td>\n","      <td>Australia</td>\n","      <td>Deluxe Double, Window</td>\n","      <td>6</td>\n","      <td>Feb-25</td>\n","      <td>Couple</td>\n","      <td>8.0</td>\n","      <td>Need room makeup‚Ä¶ every stay..</td>\n","      <td>we are regularlythe hotel need to overhaul\\nth...</td>\n","      <td>Paid extra for little bit big  View.. with Win...</td>\n","      <td>A Hotel Joo Chiat</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Mark</td>\n","      <td>Malaysia</td>\n","      <td>Superior Twin, No Window</td>\n","      <td>1</td>\n","      <td>Nov-24</td>\n","      <td>Solo traveller</td>\n","      <td>8.0</td>\n","      <td>CH</td>\n","      <td>Good location, nearby shops and walking distan...</td>\n","      <td>Room light, not bright enough.</td>\n","      <td>A Hotel Joo Chiat</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   review_ID reviewer_name reviewer_country            review_room_name  \\\n","0          1      Gabriela    United States         Deluxe Twin, Window   \n","1          2             A        Singapore  Superior Double, No Window   \n","2          3       Sumarni        Australia    Superior Twin, No Window   \n","3          4        Samsul        Australia       Deluxe Double, Window   \n","4          5          Mark         Malaysia    Superior Twin, No Window   \n","\n","   review_num_nights review_stay_date review_traveler_type  review_score  \\\n","0                  2           Jan-25               Family          10.0   \n","1                  4           Feb-25               Couple           7.0   \n","2                  3           Feb-25                Group           8.0   \n","3                  6           Feb-25               Couple           8.0   \n","4                  1           Nov-24       Solo traveller           8.0   \n","\n","                                        review_title  \\\n","0                         Comfortable and convenient   \n","1  Overall I'm quite happy with the stay except f...   \n","2                    Good and achieve my expectation   \n","3                     Need room makeup‚Ä¶ every stay..   \n","4                                                 CH   \n","\n","                                review_positive_text  \\\n","0  This was a last minute booking so we arrived t...   \n","1                                                NaN   \n","2                                                NaN   \n","3  we are regularlythe hotel need to overhaul\\nth...   \n","4  Good location, nearby shops and walking distan...   \n","\n","                                review_negative_text         hotel_name  \\\n","0                                                NaN  A Hotel Joo Chiat   \n","1                A little costly for a budget hotel.  A Hotel Joo Chiat   \n","2  The fire alarm and no communication with the p...  A Hotel Joo Chiat   \n","3  Paid extra for little bit big  View.. with Win...  A Hotel Joo Chiat   \n","4                     Room light, not bright enough.  A Hotel Joo Chiat   \n","\n","  detected_lang_title detected_lang_pos detected_lang_neg  removal_reason  \n","0                  en                en               NaN             NaN  \n","1                  en                en                en             NaN  \n","2                  en                en                en             NaN  \n","3                  en                en                en             NaN  \n","4                  en                en                en             NaN  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import ast\n","from transformers import pipeline, AutoTokenizer\n","\n","# Load dataset\n","booking_reviews = pd.read_csv('booking_reviews_clean.csv', dtype={'hotel_id':str})\n","booking_reviews.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sS0MqReciAT0"},"outputs":[],"source":["# Set up sentiment pipeline and tokenizer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n","from tqdm import tqdm\n","\n","\n","# For sentiment analysis (must match DistilBERT model)\n","sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","sentiment_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n","sentiment_pipeline = pipeline(\"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_tokenizer)\n","\n","# For token-based chunking (matches embedding model)\n","chunking_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n","MAX_TOKENS = 150\n","\n","# Sentence tokenizer fallback (regex-based)\n","def simple_sentence_tokenizer(text):\n","    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', str(text)) if s.strip()]\n","\n","# Dynamic chunker based on token length\n","def chunk_sentences_by_token_limit(sentences, max_tokens=MAX_TOKENS):\n","    chunks = []\n","    current_chunk = []\n","    current_tokens = 0\n","\n","    for sentence in sentences:\n","        sentence_tokens = len(chunking_tokenizer.encode(sentence, add_special_tokens=False))\n","\n","        if current_tokens + sentence_tokens > max_tokens and current_chunk:\n","            chunks.append(' '.join(current_chunk))\n","            current_chunk = [sentence]\n","            current_tokens = sentence_tokens\n","        else:\n","            current_chunk.append(sentence)\n","            current_tokens += sentence_tokens\n","\n","    if current_chunk:\n","        chunks.append(' '.join(current_chunk))\n","\n","    return chunks\n","\n","\n","# Function to process each review row\n","def chunk_review_with_sentiment_title(row):\n","    review_id = row['review_ID']\n","\n","    title_text = str(row['review_title']) if pd.notnull(row['review_title']) else \"\"\n","    positive_text = str(row['review_positive_text']) if pd.notnull(row['review_positive_text']) else \"\"\n","    negative_text = str(row['review_negative_text']) if pd.notnull(row['review_negative_text']) else \"\"\n","\n","    # Classify title sentiment and tokenise\n","    title_sent = sentiment_pipeline(title_text)[0]['label']\n","    title_sentences = simple_sentence_tokenizer(title_text)\n","    positive_sentences = simple_sentence_tokenizer(positive_text)\n","    negative_sentences = simple_sentence_tokenizer(negative_text)\n","\n","    # Merge title sentences into sentiment group\n","    if title_sent == 'POSITIVE':\n","        positive_sentences = title_sentences + positive_sentences\n","    else:\n","        negative_sentences = title_sentences + negative_sentences\n","\n","    # Group by sentiment using token-based chunking\n","    chunks = []\n","    for sentiment, sentences in [('Positive', positive_sentences), ('Negative', negative_sentences)]:\n","        grouped_chunks = chunk_sentences_by_token_limit(sentences, max_tokens=MAX_TOKENS)\n","        for i, chunk in enumerate(grouped_chunks):\n","            chunks.append({\n","                'review_id': review_id,\n","                'chunk_id': f\"{review_id}_{sentiment}_{i}\",\n","                'hotel_name': row['hotel_name'],\n","                'review_score': row['review_score'],\n","                'sentiment': sentiment,\n","                'chunk_text': chunk,\n","                'reviewer_name': row['reviewer_name'],\n","                'reviewer_country': row['reviewer_country'],\n","                'review_room_name': row['review_room_name'],\n","                'review_num_nights': row['review_num_nights'],\n","                'review_stay_date': row['review_stay_date'],\n","                'review_traveler_type': row['review_traveler_type']\n","            })\n","\n","    return chunks\n","\n","# Apply to your DataFrame\n","processed_chunks = []\n","for _, row in tqdm(booking_reviews.iterrows(), total=len(booking_reviews), desc=\"Processing reviews\"):\n","    processed_chunks.extend(chunk_review_with_sentiment_title(row))\n","\n","# Convert to DataFrame and export\n","chunks_df = pd.DataFrame(processed_chunks)\n","chunks_df.to_csv(\"review_chunks_with_sentiment.csv\", index=False)\n","chunks_df.head(10)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJ_5rtxPiAT1"},"outputs":[],"source":["# --------------------------\n","# Remove rows with short reviews\n","# --------------------------\n","\n","# Load dataset\n","review_chunks_with_sentiment = pd.read_csv('review_chunks_with_sentiment.csv')\n","\n","# Filter out rows with short reviews (less than or equal to 3 words)\n","filtered_df = review_chunks_with_sentiment[review_chunks_with_sentiment['chunk_text'].str.split().apply(len) <= 3]\n","print(f\"Number of rows in filtered_df: {len(filtered_df)}\")\n","print(f\"Number of unique values: {len(filtered_df['chunk_text'].unique())}\")\n","\n","# Save clean file\n","review_chunks_with_sentiment_clean = review_chunks_with_sentiment.drop(filtered_df.index)\n","review_chunks_with_sentiment_clean.to_csv(\"review_chunks_with_sentiment_clean.csv\", index=False)\n","\n","print(f\"\\nNumber of rows in original review_chunks_with_sentiment: {len(review_chunks_with_sentiment)}\")\n","print(f\"Number of rows after filtering: {len(review_chunks_with_sentiment_clean)}\")\n","\n","print(\"Dataset saved to review_chunks_with_sentiment_clean.csv\")"]},{"cell_type":"markdown","metadata":{"id":"llMnKcDliAT2"},"source":["### 3. Prep for Weaviate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYlYujw5iAT3"},"outputs":[],"source":["import pandas as pd\n","\n","# ---- Flatten Review Scores ----\n","def stringify_review_scores(scores):\n","    if pd.isna(scores): return \"\"\n","    if isinstance(scores, str):\n","        try: scores = ast.literal_eval(scores)\n","        except: return str(scores)\n","    return \". \".join([f\"{s['category']}: {s['score']}\" for s in scores])\n","\n","\n","# ---- Flatten Cot / Extra Bed Policies ----\n","def stringify_bed_policies_safe(policies):\n","    if pd.isna(policies): return \"\"\n","    try:\n","        if isinstance(policies, str):\n","            policies = ast.literal_eval(policies)\n","        if not isinstance(policies, dict):\n","            return str(policies)\n","    except:\n","        return str(policies)\n","\n","    flat = []\n","    for age, options in policies.items():\n","        if isinstance(options, dict):\n","            details = ', '.join([f\"{k} - {v}\" for k, v in options.items()])\n","            flat.append(f\"{age}: {details}\")\n","        else:\n","            flat.append(f\"{age}: {options}\")\n","    return \". \".join(flat)\n","\n","\n","# ---- Flatten Payment Methods ----\n","def stringify_payment_methods(pm):\n","    if pd.isna(pm): return \"\"\n","    if isinstance(pm, str):\n","        try: pm = ast.literal_eval(pm)\n","        except: return str(pm)\n","    cards = \", \".join(pm.get(\"accepted_cards\", []))\n","    cash = pm.get(\"cash_accepted\", \"Unknown\")\n","    return f\"Accepted Cards: {cards}. Cash Accepted: {cash}\"\n","\n","\n","# ---- Flatten Room Details ----\n","def stringify_room_details(rooms):\n","    if pd.isna(rooms): return \"\"\n","    if isinstance(rooms, str):\n","        try: rooms = ast.literal_eval(rooms)\n","        except: return str(rooms)\n","    summary = []\n","    for r in rooms:\n","        desc = f\"Room: {r.get('Room Name')}, Size: {r.get('Room Size')}, Features: {r.get('Room Highlights & Facilities', '')}\"\n","        summary.append(desc)\n","    return \". \".join(summary)\n","\n","\n","# ---- Flatten Surroundings ----\n","def stringify_surroundings(surr):\n","    if pd.isna(surr): return \"\"\n","    if isinstance(surr, str):\n","        try: surr = ast.literal_eval(surr)\n","        except: return str(surr)\n","    flat = []\n","    for cat, items in surr.items():\n","        entries = \", \".join([f\"{i['name']} ({i.get('distance', 'n/a')})\" for i in items if isinstance(i, dict) and 'name' in i])\n","        flat.append(f\"{cat}: {entries}\")\n","    return \". \".join(flat)\n","\n","# ---- Flatten Facilities ----\n","def stringify_facilities(facilities):\n","    if pd.isna(facilities): return \"\"\n","    if isinstance(facilities, str):\n","        try: facilities = ast.literal_eval(facilities)\n","        except: return str(facilities)\n","    flat = []\n","    for cat, items in facilities.items():\n","        flat.append(f\"{cat}: {', '.join(items)}\")\n","    return \". \".join(flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZmUBXt2iAT4","outputId":"79d57876-1bb8-4bd2-a23e-59d5a7c2ec60"},"outputs":[{"name":"stdout","output_type":"stream","text":["Property data saved to property_df_flat.csv\n"]}],"source":["property_df = pd.read_csv('property_df.csv')\n","review_chunks = pd.read_csv('review_chunks_with_sentiment_clean.csv')\n","\n","# Working on a copy of property_data\n","property_df_flat = property_df.copy()\n","\n","# ---- Apply functions ----\n","property_df_flat[\"review_scores_text\"] = property_df_flat[\"review_scores\"].apply(stringify_review_scores)\n","property_df_flat[\"cot_extra_bed_policies_text\"] = property_df_flat[\"cot_extra_bed_policies\"].apply(stringify_bed_policies_safe)\n","property_df_flat[\"payment_methods_text\"] = property_df_flat[\"payment_methods\"].apply(stringify_payment_methods)\n","property_df_flat[\"room_details_text\"] = property_df_flat[\"room_details\"].apply(stringify_room_details)\n","property_df_flat[\"surroundings_text\"] = property_df_flat[\"surroundings\"].apply(stringify_surroundings)\n","property_df_flat[\"facilities_text\"] = property_df_flat[\"facilities\"].apply(stringify_facilities)\n","\n","# Ensure num_reviews is an integer (not float)\n","property_df_flat[\"num_reviews\"] = property_df_flat[\"num_reviews\"].fillna(0).astype(int)\n","\n","# Drop original cols\n","property_df_flat = property_df_flat.drop(\n","    columns=[\"review_scores\", \"cot_extra_bed_policies\", \"payment_methods\", \"room_details\", \"surroundings\", \"facilities\"]\n",")\n","\n","property_df_flat.to_csv(\"property_df_flat.csv\", index=False)\n","print(\"Property data saved to property_df_flat.csv\")"]},{"cell_type":"markdown","metadata":{"id":"PgRGUynziAT5"},"source":["### 4. Compute Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWTElbAJiAT5","outputId":"8a20dabd-136f-4a7d-fcf1-c3bd3f98f668","colab":{"referenced_widgets":["d4094199deeb4a0eb66c1f0ddab67d36"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4094199deeb4a0eb66c1f0ddab67d36","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/4583 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293302/293302 [00:00<00:00, 1301982.56it/s]\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Load your CSV\n","df = pd.read_csv(\"review_chunks_with_sentiment_clean.csv\")\n","\n","# Load model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Compute embeddings\n","df['embedding'] = list(tqdm(model.encode(df['chunk_text'].tolist(), batch_size=64, show_progress_bar=True)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr6dJuMSiAT5","outputId":"a04871fc-7be9-4ea0-91fa-f24d84e6dcea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Embeddings saved to review_chunks_with_sentiment_embeddings.csv\n"]}],"source":["# Save the DataFrame with embeddings to a new CSV file\n","df.to_csv(\"review_chunks_with_sentiment_embeddings.csv\", index=False)\n","print(\"Embeddings saved to review_chunks_with_sentiment_embeddings.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yUeuKi1iAT6"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}